{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all\n",
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import necessary library such as math, nltk, bigram, and collections.\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import random\n",
    "from random import shuffle\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose news domain as our dataset\n",
    "best2010=[]\n",
    "fp= io.open('BEST2010/news.txt','r',encoding='utf-8')\n",
    "for i,line in enumerate(fp):\n",
    "    best2010.append(line.strip()[:-1])\n",
    "fp.close()\n",
    "all_vocabulary =set()\n",
    "total_word_count =0\n",
    "for line in best2010:\n",
    "    for word in line.split('|'):        \n",
    "        all_vocabulary.add(word)\n",
    "        total_word_count+=1\n",
    "sentences = best2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_sentence = [sentence.split(\"|\") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 11776\n",
      "['<unk>', '<eos>', ' ', 'ที่', 'การ', 'ว่า', 'มี', 'ใน', 'และ', 'ได้']\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenize_sentence,min_freq = 3)\n",
    "vocab.insert_token('<unk>', 0) #add <unk> token to index 0\n",
    "vocab.insert_token('<eos>', 1) #add <eos> token to index 1\n",
    "# vocab.insert_token('<pad>', 2) #add <eos> token to index 2\n",
    "vocab.set_default_index(vocab['<unk>']) #make index 0 the default index (when encountering unknown words)\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")                         \n",
    "print(vocab.get_itos()[:10]) #get first 10 words  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./error.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['จะ', 'กลับ', 'ไป', 'แก้', 'ใข้', 'ใน', 'สิ่ง', 'ที่ผ่าน', 'มา']\n",
      "index :  [3, 5] WRONG :  ['แก้', 'ใข้']  CORRECT :  ['แก้ไข']  CLASSS :  misspelled\n",
      "['จะ', 'เปลี่ยน', 'ไป', 'ใช้', 'อีก', 'เบอร์', 'ต้อง', 'ทำ', 'ยังไง', 'ค่ะ']\n",
      "index :  [9, 10] WRONG :  ['ค่ะ']  CORRECT :  ['คะ']  CLASSS :  misspelled\n",
      "['อยาก', 'ได้', 'ดงิน', 'คืน', 'อ่ะ', 'ค่ะ']\n",
      "index :  [2, 3] WRONG :  ['ดงิน']  CORRECT :  ['เงิน']  CLASSS :  misspelled\n",
      "index :  [4, 6] WRONG :  ['อ่ะ', 'ค่ะ']  CORRECT :  ['อะ', 'คะ']  CLASSS :  misspelled\n",
      "['เห้อ', ' ', 'เหนื่อยใจ']\n",
      "index :  [0, 1] WRONG :  ['เห้อ']  CORRECT :  ['เฮ้อ']  CLASSS :  misspelled\n",
      "['เสียดาย', 'อ่ะ', ' ', 'ไม่', 'งัน', 'พี่', 'ปุ๊ก', 'ได้', 'ปิด', 'ซอย', 'เลี้ยง', 'แล้ว']\n",
      "index :  [1, 2] WRONG :  ['อ่ะ']  CORRECT :  ['อะ']  CLASSS :  misspelled\n"
     ]
    }
   ],
   "source": [
    "for i in range(30,35):\n",
    "    print(df[1][i])\n",
    "    sent_ = df[1][i]\n",
    "    list_wrong_word = df[2][i]\n",
    "    for idx,right_word,wrong_class in list_wrong_word:\n",
    "        print(\"index : \",idx,\"WRONG : \",sent_[idx[0]:idx[1]],\" CORRECT : \",right_word,\" CLASSS : \",wrong_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see this dataset will have the sentence with som wrong words that needs to be correct \n",
    "1. End-to-end network : use this as a ground truth for correcting the wrong word  where the input can be a whole sentence or each words\n",
    "2. We can input a whole sentence and network will predict where is the index of wrong word and correct it combine with dictionary\n",
    "3. We will use this as testset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To detect where the mis-spelling is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the correct labels and wrong labels for each token\n",
    "def generate_label_index(df,k):\n",
    "    sent_ = df[1][k]\n",
    "    list_wrong_word = df[2][k]\n",
    "    labels = [0 for _ in range(len(sent_))]\n",
    "    for idx,_,_ in list_wrong_word:\n",
    "        for i in range(idx[0],idx[1]):\n",
    "            labels[i] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE :  ['หริอ', 'มี', 'ปัญหา', 'อะไร', 'ช่วย', 'ตอบ', 'ด้วย', 'ครับ']\n",
      "LABELS :  [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['อยาก', 'สมัคร', ' ', 'sms', ' ', 'เงิน', 'เข้า', 'เงิน', 'ออก', 'ทาง', 'เน็ต', 'ต้อง', 'ทำ', 'ยังงัย']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ไม่', 'ทราบ', 'ว่า', 'คุณ', ' ', 'พัชชาา', ' ', 'ไม่', 'สน', 'โลก', ' ', 'เปลี่ยน', 'เฉพาะ', 'เครื่อง', ' ', 'หรือ', 'เปลี่ยน', 'ทั้ง', 'เครื่อง', 'และ', 'เบอร์', 'คะ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ทำ', 'รายการ', 'ที่', 'ตู้', 'atm', ' ', 'อะ', 'คะ', ' ', 'เป็น', 'การ', 'ทำ', 'รายการ', 'เปลี่ยน', 'เบอ', 'โทรสับ', 'อะ', 'คะ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ย่า', 'ยัง', 'ดี', 'ที่', 'สิริ', 'ตอบ', 'แบบ', 'นั้น', ' ', 'ดู', 'ของ', 'เค้า', 'ดิ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['จขกท', '.', ' ', '3', '3', '3', '3', '3', '3', '3', ' ', 'เจ้าของ', 'เม้นท์', 'นี้', ' ', '9', '9', '9', '9', '9', '9', ' ', ' ', 'บุพเพสันนิวาสรึเปล่า', 'เนี่ย']\n",
      "LABELS :  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ละ', 'เวลา', 'เชค', 'ยอด', 'เงิน', 'ใน', 'บัญชี', 'ด้วย', 'คะ']\n",
      "LABELS :  [0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['เล็ก', 'ๆ', 'ไม่', 'ใหญ่', 'ๆ', 'นาง', 'ทำ', 'ได้', ' ', ' ', 'น้องพี', 'สู้', 'ๆ', ' ', 'หลบ', 'ไป', 'นอน', 'หน่อย', 'เถอะ', 'นะ', ' ', 'เด๋ว', 'ตาคล้ำไม่สวยนาจา', '.', '.', '.']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ยอดพย', ' ', 'รวม', 'พวก', ' ', 'LTF', ' ', 'RMF', ' ', 'ไหม', 'ครับ', ' ', 'เดือน', 'นั้น', 'ใส่', 'เงิน', 'ไป', 'เยอะ', 'เลย', ' ']\n",
      "LABELS :  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ตกลง', ' ', 'ยังงัย', '?', '?', 'ให้', 'ข้อมูล', 'ได้', 'ใหม', 'ค่ะ']\n",
      "LABELS :  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['อีก', 'เบอ']\n",
      "LABELS :  [0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ไ ทยพาณิช']\n",
      "LABELS :  [1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['พอ', 'มี', 'ลิ้งค์', 'สิทธิประโยชน์', 'ของ', 'ผู้', 'ถือ', 'บัตร', 'เดบิต', ' ', 'โดย', 'เฉพาะ', 'เรื่อง', 'โอน', 'เงิน', 'มั๊ย', 'ครับ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ชูวิท']\n",
      "LABELS :  [1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['เธอ', 'ประดุจ', 'ดั่ง', 'เทพธิดา', ' ', 'นางฟ้า', ' ', 'คง', 'ซิ', ' ', 'เอื้อมบ่ถึง', ' ', 'แม่นางไม้', ' ', 'ด่อก', ' ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['รหัส', 'เข้า', 'โอน', 'เงิน', 'ที่', 'มือถือ', 'อ่ะ', 'ครับ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['เขา', 'ให้', 'มา', 'ถาม', 'ทาง', 'ธนาคาร', 'เลย', 'อะ', 'ครับ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['วัน', 'นี้', 'เมื่อ', ' ', '4', ' ', 'ปี', 'ก่อน', ' ', 'ตีมมิ่งล่ม', ' ', 'วัน', 'นี้', 'พันธิป', 'จะ', 'ล่ม', 'แทน', 'สิ', 'นะ', 'ครับ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['รบกวน', 'สอบถามึะ', ' ', 'โอน', 'เงิน', 'ผ่าน', 'ระบบ', 'พร้อมเพ', ' ', 'ทำไม', 'ยอด', 'ไม่', 'เข้า', ' ', 'ปลายทาง', ' ', 'ทั้งที่', 'ยอด', 'ก้', 'ตัด', 'ไป', ' ', 'แล้ว', ' ', 'นาน', 'ไป', 'ไหม']\n",
      "LABELS :  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['แต่ง', 'หน้า', 'ผี', 'ให้', 'น่า', 'กลัว', 'ที่สุด', ' ', 'เอ็ฟเฟ็ค', 'แบบ', 'จัด', 'เต็ม', 'ระดับ', 'ฮอลิวู้ด', ' ', 'แล้ว', 'ไป', 'หา', 'เพื่อนกลาง', 'ดึก', 'เคาะ', 'ประตู', 'เพื่อ', 'บอก', 'ว่า', ' ', 'Happy Haloween']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['เค้า', 'เรียก', 'แก้ม', 'เชอรี่', ' ', ' ', 'เกาหลี', 'งิ', 'ชอบ', 'กัน', 'ตรึม']\n",
      "LABELS :  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['สงสาร', 'เก๊า', 'เถอะ', 'นะ', ' ', 'อาบ', 'น้ำ', 'ไป', 'ด้วย', ' ', 'F', '5', ' ', 'ไป', 'ด้วย', 'สามารถ', 'สุด', 'เท่า', 'นี้', ' ', 'ฮือออ']\n",
      "LABELS :  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ฝาก', 'เิน', 'โอน', 'เงิน', 'แยก', 'กัน', 'กะ', 'บัตร', 'เก่า', 'ใช่', 'ไหม', 'คะ']\n",
      "LABELS :  [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['อ่อ', ' ', 'ขอบคุณ', 'นะ', 'คพ']\n",
      "LABELS :  [1, 0, 0, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ใส่', 'เบอ', 'รหัส', 'ที', 'ส่ง', 'มา', 'แรว', 'รหัส', 'ยืนยัน', 'เสด', 'ก้', 'อม่ด้']\n",
      "LABELS :  [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ขอโทษ', 'น่ะ', 'ครับ', ' ', 'ขอ', 'สอบถาม', 'หน่อย', 'ครับ', ' ', 'คือ', 'เข้า', 'สู่', 'ระบบ', 'k-mobile  banking plus', ' ', 'ยังไง', 'อะ', 'ครับ']\n",
      "LABELS :  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ขอโทษ', 'นะ', 'ครับ', 'กรณี', 'บัตร', 'หาย', 'สามารรถ', 'ขอ', 'รับ', 'บัตร', 'โดเรมอน', 'ได้', 'ยุ', 'ไหม', 'คับ']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "\n",
      "\n",
      "SENTENCE :  ['บร่ะ', 'แล้ว']\n",
      "LABELS :  [1, 0]\n",
      "\n",
      "\n",
      "SENTENCE :  ['ตอน', 'นี้', 'เรา', 'มี', 'ทะเล', 'อยู่', 'หน้า', 'บ้าน', 'และ', 'มี', 'สระ', 'น้ำ', 'อยู่', 'ใน', 'บ้าน', ' ', ' ', ' ', 'ฟินส์', 'จริงจริ๊ง']\n",
      "LABELS :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(29):\n",
    "    print(\"SENTENCE : \",df[1][k])\n",
    "    print(\"LABELS : \",generate_label_index(df,k))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To detect real word spelling error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list_string = []+df[1][0]\n",
    "for i in range(len(df)):\n",
    "    all_list_string += df[1][i]\n",
    "all_list_string = \"\".join(all_list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 139 unique chars in the data set\n",
      "[' ' '!' '\"' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5'\n",
      " '6' '7' '9' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'K' 'L' 'M' 'N' 'O'\n",
      " 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Z' '^' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
      " 'i' 'k' 'l' 'm' 'n' 'o' 'p' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'ก' 'ข'\n",
      " 'ฃ' 'ค' 'ฆ' 'ง' 'จ' 'ฉ' 'ช' 'ซ' 'ญ' 'ฐ' 'ฑ' 'ฒ' 'ณ' 'ด' 'ต' 'ถ' 'ท' 'ธ'\n",
      " 'น' 'บ' 'ป' 'ผ' 'ฝ' 'พ' 'ฟ' 'ภ' 'ม' 'ย' 'ร' 'ฤ' 'ล' 'ว' 'ศ' 'ษ' 'ส' 'ห'\n",
      " 'อ' 'ฮ' 'ฯ' 'ะ' 'ั' 'า' 'ำ' 'ิ' 'ี' 'ึ' 'ื' 'ุ' 'ู' 'ฺ' 'เ' 'แ' 'โ' 'ใ'\n",
      " 'ไ' 'ๅ' 'ๆ' '็' '่' '้' '๊' '๋' '์' 'ํ' '๐' '๒' '๖']\n"
     ]
    }
   ],
   "source": [
    "np_str = np.array(list(all_list_string))\n",
    "all_char = np.unique(np_str)\n",
    "\n",
    "sorted(all_char)\n",
    "print(\"There are %d unique chars in the data set\" % len(all_char))\n",
    "print(all_char)\n",
    "char_map = dict(zip(all_char, range(len(all_char))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example String to feature conversion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['หริอ', 'มี', 'ปัญหา', 'อะไร', 'ช่วย', 'ตอบ', 'ด้วย', 'ครับ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 1., 2., 3., 0.,\n",
       "       0., 2., 0., 0., 0., 2., 3., 0., 0., 1., 2., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_str(string):\n",
    "    global all_char, char_map\n",
    "    result = np.zeros(len(all_char))\n",
    "    np_str = np.array(list(string))\n",
    "    str_char, str_char_count = np.unique(np_str, return_counts=True)\n",
    "    for char, count in zip(str_char, str_char_count):\n",
    "        result[char_map[char]] = count\n",
    "    return result\n",
    "\n",
    "# run example feature transformation\n",
    "print(\"Example String to feature conversion\")\n",
    "display(df[1][0])\n",
    "display(count_str(\"\".join(df[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a correct version and vocab by \n",
    "# replace wrong word into the list \n",
    "# and make vocab from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "Word embbeding shape  torch.Size([15597, 108])\n",
      "Character feature shape torch.Size([15597, 139])\n",
      "Concatenate feature shape torch.Size([15597, 247])\n",
      "label shape (15597,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nontapat_s\\AppData\\Local\\Temp\\ipykernel_9280\\2651019436.py:12: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  x_f1 = torch.LongTensor([[e for e in sl] for sl in temp])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def data2features(data,dict_):\n",
    "  features = [torch.LongTensor(dict_(sentence)) for sentence in data]\n",
    "  return features\n",
    "\n",
    "data = df.to_numpy()\n",
    "\n",
    "x_ = data2features(data[:,1],vocab)\n",
    "x_ = pad_sequence(x_, batch_first=True)\n",
    "\n",
    "temp = [np.vectorize(count_str, otypes=[object])(\"\".join(data[:,1][k])) for k in range(len(data))]\n",
    "x_f1 = torch.LongTensor([[e for e in sl] for sl in temp])\n",
    "\n",
    "label = np.array([generate_label_index(df,k) for k in range(len(data))],dtype=object)\n",
    "data_embed = torch.cat((x_,x_f1),1)\n",
    "print(\"Data\")\n",
    "print(\"Word embbeding shape \",x_.shape)\n",
    "print(\"Character feature shape\", x_f1.shape)\n",
    "print(\"Concatenate feature shape\", data_embed.shape)\n",
    "print(\"label shape\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for \"Action\" Classification Task\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(data_embed, label, test_size=0.30, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size:  torch.Size([10917, 247])\n",
      "Training labels size:  (10917,)\n",
      "Validation dataset size:  torch.Size([2340, 247])\n",
      "Validation labels size:  (2340,)\n",
      "Testing dataset size:  torch.Size([2340, 247])\n",
      "Testing labels size:  (2340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset size: \",X_train.shape)\n",
    "print(\"Training labels size: \",y_train.shape)\n",
    "print(\"Validation dataset size: \",X_val.shape)\n",
    "print(\"Validation labels size: \",y_val.shape)\n",
    "print(\"Testing dataset size: \",X_test.shape)\n",
    "print(\"Testing labels size: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Embedding, Linear, Dropout\n",
    "import torch.nn.functional as F \n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ErrorDetect(Dataset):\n",
    "  def __init__(self, data, labels): \n",
    "    self.data = data\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    return self.data[idx],self.labels[idx]\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.data) \n",
    "\n",
    "\n",
    "train_dataset = ErrorDetect(X_train,) \n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW for predict the right word at the error index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Candidate by Character n-gram inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google --> (tri-grams) --> goo oog ogl gle\n",
    "# if we consider goo --> google good good-bye \n",
    "# this can simply find what the word cause this tri-grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
